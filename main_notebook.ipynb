{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "I am trying to reimplement a PyTorch code of doing Deep One-Class Classification based on the paper Deep SVDD using PyTorch. In my reimplementation, I will mainly using "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "81ae14e5b48f6568"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-11T07:57:40.689913300Z",
     "start_time": "2023-10-11T07:57:29.796351800Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, Dataset, Subset\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor, Normalize, Compose, Lambda\n",
    "from torch.nn.functional import normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "efccdfdb8c94e100",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-11T04:06:47.999378400Z",
     "start_time": "2023-10-11T04:06:47.858756700Z"
    }
   },
   "outputs": [],
   "source": [
    "# Download MNIST Dataset\n",
    "\n",
    "training_data = datasets.MNIST(\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=ToTensor(),\n",
    ")\n",
    "\n",
    "test_data = datasets.MNIST(\n",
    "    root=\"data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=ToTensor(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "83b3af629da6857a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-11T04:06:50.115510200Z",
     "start_time": "2023-10-11T04:06:50.084250Z"
    }
   },
   "outputs": [],
   "source": [
    "# Set hardware\n",
    "\n",
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "17e6cc4336515033",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-11T04:06:51.119909500Z",
     "start_time": "2023-10-11T04:06:51.073044100Z"
    }
   },
   "outputs": [],
   "source": [
    "class MNIST_LeNet_AutoEncoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.rep_dim = 32\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        \n",
    "        # Encoder: Same as Deep Out-of-Context (OOC) network\n",
    "        self.conv1 = nn.Conv2d(1, 8, 5, bias=False, padding=2)\n",
    "        self.bn1 = nn.BatchNorm2d(8, eps=1e-04, affine=False)\n",
    "        self.conv2 = nn.Conv2d(8, 4, 5, bias=False, padding=2)\n",
    "        self.bn2 = nn.BatchNorm2d(4, eps=1e-04, affine=False)\n",
    "        self.fc1 = nn.Linear(4 * 7 * 7, self.rep_dim, bias=False)\n",
    "        \n",
    "        # Decoder\n",
    "        self.deconv1 = nn.ConvTranspose2d(2, 4, 5, bias=False, padding=2)\n",
    "        self.bn3 = nn.BatchNorm2d(4, eps=1e-04, affine=False)\n",
    "        self.deconv2 = nn.ConvTranspose2d(4, 8, 5, bias=False, padding=3)\n",
    "        self.bn4 = nn.BatchNorm2d(8, eps=1e-04, affine=False)\n",
    "        self.deconv3 = nn.ConvTranspose2d(8, 1, 5, bias=False, padding=2)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.pool(nn.LeakyReLU(self.bn1(x)))\n",
    "        x = self.conv2(x)\n",
    "        x = self.pool(nn.LeakyReLU(self.bn2(x)))\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc1(x)\n",
    "        x = x.view(x.size(0), int(self.rep_dim / 16), 4, 4)\n",
    "        x = nn.functional.interpolate(nn.LeakyReLU(x), scale_factor=2)\n",
    "        x = self.deconv1(x)\n",
    "        x = nn.functional.interpolate(nn.LeakyReLU(self.bn3(x)), scale_factor=2)\n",
    "        x = self.deconv2(x)\n",
    "        x = nn.functional.interpolate(nn.LeakyReLU(self.bn4(x)), scale_factor=2)\n",
    "        x = self.deconv3(x)\n",
    "        x = torch.sigmoid(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ed0b570028820f00",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-11T04:06:51.838187200Z",
     "start_time": "2023-10-11T04:06:51.775691300Z"
    }
   },
   "outputs": [],
   "source": [
    "class MNIST_LeNet_Network(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.rep_dim = 32\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(1, 8, 5, bias=False, padding=2)\n",
    "        self.bn1 = nn.BatchNorm2d(8, eps=1e-04, affine=False)\n",
    "        self.conv2 = nn.Conv2d(8, 4, 5, bias=False, padding=2)\n",
    "        self.bn2 = nn.BatchNorm2d(4, eps=1e-04, affine=False)\n",
    "        self.fc1 = nn.Linear(4 * 7 * 7, self.rep_dim, bias=False)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.pool(nn.LeakyReLU(self.bn1(x)))\n",
    "        x = self.conv2(x)\n",
    "        x = self.pool(nn.LeakyReLU(self.bn2(x)))\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc1(x)\n",
    "        return x        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "def254a9b8e0d751",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-11T04:06:52.849264200Z",
     "start_time": "2023-10-11T04:06:52.802349400Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MNIST_LeNet_Network(\n",
      "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv1): Conv2d(1, 8, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)\n",
      "  (bn1): BatchNorm2d(8, eps=0.0001, momentum=0.1, affine=False, track_running_stats=True)\n",
      "  (conv2): Conv2d(8, 4, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)\n",
      "  (bn2): BatchNorm2d(4, eps=0.0001, momentum=0.1, affine=False, track_running_stats=True)\n",
      "  (fc1): Linear(in_features=196, out_features=32, bias=False)\n",
      ")\n",
      "MNIST_LeNet_AutoEncoder(\n",
      "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv1): Conv2d(1, 8, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)\n",
      "  (bn1): BatchNorm2d(8, eps=0.0001, momentum=0.1, affine=False, track_running_stats=True)\n",
      "  (conv2): Conv2d(8, 4, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)\n",
      "  (bn2): BatchNorm2d(4, eps=0.0001, momentum=0.1, affine=False, track_running_stats=True)\n",
      "  (fc1): Linear(in_features=196, out_features=32, bias=False)\n",
      "  (deconv1): ConvTranspose2d(2, 4, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)\n",
      "  (bn3): BatchNorm2d(4, eps=0.0001, momentum=0.1, affine=False, track_running_stats=True)\n",
      "  (deconv2): ConvTranspose2d(4, 8, kernel_size=(5, 5), stride=(1, 1), padding=(3, 3), bias=False)\n",
      "  (bn4): BatchNorm2d(8, eps=0.0001, momentum=0.1, affine=False, track_running_stats=True)\n",
      "  (deconv3): ConvTranspose2d(8, 1, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "net = MNIST_LeNet_Network().to(device)\n",
    "print(net)\n",
    "ae = MNIST_LeNet_AutoEncoder().to(device)\n",
    "print(ae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eb118a7a0836d8b9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-11T04:08:17.129289100Z",
     "start_time": "2023-10-11T04:08:17.019914200Z"
    }
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (527837196.py, line 9)",
     "output_type": "error",
     "traceback": [
      "\u001B[1;36m  Cell \u001B[1;32mIn[8], line 9\u001B[1;36m\u001B[0m\n\u001B[1;33m    def GlobalContrastNormalization(torch.tensor(tensor), scale='l2'):\u001B[0m\n\u001B[1;37m                                         ^\u001B[0m\n\u001B[1;31mSyntaxError\u001B[0m\u001B[1;31m:\u001B[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# Normalization of the MNIST Dataset\n",
    "\n",
    "# Calculate the mean and standard deviation of the pixel values in the training dataset\n",
    "train_data = torch.stack([t[0] for t in training_data])\n",
    "mean = train_data.mean()\n",
    "std = train_data.std()\n",
    "\n",
    "# Define a new PyTorch transform that performs the same operation as the numpy-based GCN.\n",
    "def GlobalContrastNormalization(tensor, scale='l2'):\n",
    "    assert scale in ('l1', 'l2')\n",
    "    n_features = int(torch.prod(tensor.shape))\n",
    "            \n",
    "    tensor = tensor - tensor.mean()\n",
    "    \n",
    "    if (scale == 'l1'):\n",
    "        tensor = tensor / torch.mean(torch.abs(tensor))\n",
    "    \n",
    "    if (scale == 'l2'):\n",
    "        tensor = tensor / torch.sqrt(torch.sum(tensor ** 2) / n_features)\n",
    "    \n",
    "    return tensor\n",
    "    \n",
    "    # contrast = torch.sqrt(self.lmda + (tensor**2).mean())       # calculating contrast\n",
    "    # tensor = self.s * tensor / max(contrast, self.epsilon)      # make sure contrast larger than 0\n",
    "\n",
    "# # Apply Global Contrast Normalization to the training and test datasets\n",
    "# gcn = GlobalContrastNormalization()\n",
    "# training_data.transform = Compose([ToTensor(), gcn])\n",
    "# test_data.transform = Compose([ToTensor(), gcn])\n",
    "\n",
    "transform = Compose([ToTensor(), Lambda(lambda x: GlobalContrastNormalization(x, scale='l1')), Normalize([0], [1])])\n",
    "target_transform = Lambda(lambda x: int())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "819b773df74606d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pretrain with AutoEncoder first\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(ae.parameters(), lr=0.001)\n",
    "\n",
    "# Number of epochs\n",
    "epochs = 10\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(epochs):\n",
    "    running_loss = 0.0\n",
    "    for images, _ in training_data:\n",
    "        # Move images to the device\n",
    "        images = images.to(device)\n",
    "        \n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = ae(images)\n",
    "        loss = criterion(outputs, images)\n",
    "        \n",
    "        # Backward pass and optimize\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "    \n",
    "    # Print loss after each epoch\n",
    "    print(f'Epoch {epoch+1}/{epochs}, Loss: {running_loss/len(training_data)}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
