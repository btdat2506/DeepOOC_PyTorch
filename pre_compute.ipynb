{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import logging\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch import nn, optim\n",
    "import torch.nn.functional\n",
    "from torch.utils.data import DataLoader, Dataset, Subset\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor, Normalize, Compose, Lambda\n",
    "from torch.nn.functional import normalize"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-23T04:20:38.778386900Z",
     "start_time": "2023-10-23T04:20:35.295150600Z"
    }
   },
   "id": "initial_id"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 2023-10-23 11:20:41,635 | I will output to terminal\n",
      "WARNING 2023-10-23 11:20:41,636 | FOO\n",
      "INFO 2023-10-23 11:20:41,637 | hello\n",
      "WARNING 2023-10-23 11:20:41,639 | BAR\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import logging.config\n",
    "\n",
    "logging.config.dictConfig({\n",
    "    \"version\": 1,\n",
    "    \"disable_existing_loggers\": True,\n",
    "    \"formatters\": {\n",
    "        \"default\": {\n",
    "            \"format\": \"{levelname} {asctime} | {message}\",\n",
    "            \"style\": \"{\",\n",
    "        },\n",
    "    },\n",
    "    \"handlers\": {\n",
    "        \"console\": {\n",
    "            \"class\": \"logging.StreamHandler\",\n",
    "            \"formatter\": \"default\",\n",
    "            \"stream\": sys.stdout,\n",
    "        }\n",
    "    },\n",
    "    \"root\": {\n",
    "        \"level\": \"DEBUG\",\n",
    "        \"handlers\": [\"console\"],\n",
    "    },\n",
    "})\n",
    "\n",
    "\n",
    "# Get root logger (all other loggers will be derived from this logger's\n",
    "# properties)\n",
    "logger = logging.getLogger()\n",
    "logger.warning(\"I will output to terminal\")  # No output in notebook, goes to terminal\n",
    "\n",
    "# assuming only a single handler has been setup (seems \n",
    "# to be default in notebook), set that handler to go to stdout.\n",
    "# logger.handlers[0].stream = sys.stdout\n",
    "\n",
    "logger.warning(\"FOO\")  # Prints: WARNING:root:FOO\n",
    "logger.info(\"hello\")\n",
    "\n",
    "# Other loggers derive from the root logger, so you can also do:\n",
    "logger2 = logging.getLogger(\"logger2\")\n",
    "logger2.warning(\"BAR\")  # Prints: WARNING:logger2:BAR"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-23T04:20:41.669222900Z",
     "start_time": "2023-10-23T04:20:41.631291Z"
    }
   },
   "id": "33ad622cc6a8f412"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "# Set hardware\n",
    "\n",
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-23T04:20:43.114523800Z",
     "start_time": "2023-10-23T04:20:43.091392300Z"
    }
   },
   "id": "f5e3c28b88c23a60"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "def GlobalContrastNormalization(tensor: torch.tensor, scale='l2'):\n",
    "    assert scale in ('l1', 'l2')\n",
    "    n_features = int(np.prod(tensor.shape))\n",
    "            \n",
    "    tensor = tensor - torch.mean(tensor)\n",
    "    \n",
    "    if (scale == 'l1'):\n",
    "        tensor = tensor / torch.mean(torch.abs(tensor))\n",
    "    \n",
    "    if (scale == 'l2'):\n",
    "        tensor = tensor / torch.sqrt(torch.sum(tensor ** 2) / n_features)\n",
    "    \n",
    "    return tensor"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-23T04:20:44.104778300Z",
     "start_time": "2023-10-23T04:20:44.085750400Z"
    }
   },
   "id": "249fc22063ab1e62"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "def get_target_label_idx(labels, targets):\n",
    "    \"\"\"\n",
    "    Get the indices of labels that are included in targets.\n",
    "    :param labels: array of labels\n",
    "    :param targets: list/tuple of target labels\n",
    "    :return: list with indices of target labels\n",
    "    \"\"\"\n",
    "    return [idx for idx, label in enumerate(labels) if label in targets]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-23T04:20:45.312528900Z",
     "start_time": "2023-10-23T04:20:45.281143100Z"
    }
   },
   "id": "2469a5861133f5e1"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\btdat\\AppData\\Local\\miniconda3\\envs\\Deep_OOC\\lib\\site-packages\\torchvision\\datasets\\mnist.py:65: UserWarning: train_labels has been renamed targets\n",
      "  warnings.warn(\"train_labels has been renamed targets\")\n"
     ]
    }
   ],
   "source": [
    "normal_class = 0\n",
    "\n",
    "n_classes = 2\n",
    "normal_classes = tuple([normal_class])\n",
    "outlier_classes = list(range(0, 10))\n",
    "outlier_classes.remove(normal_class)\n",
    "\n",
    "min_max = [(-0.8826567065619495, 9.001545489292527),\n",
    "           (-0.6661464580883915, 20.108062262467364),\n",
    "           (-0.7820454743183202, 11.665100841080346),\n",
    "           (-0.7645772083211267, 12.895051191467457),\n",
    "           (-0.7253923114302238, 12.683235701611533),\n",
    "           (-0.7698501867861425, 13.103278415430502),\n",
    "           (-0.778418217980696, 10.457837397569108),\n",
    "           (-0.7129780970522351, 12.057777597673047),\n",
    "           (-0.8280402650205075, 10.581538445782988),\n",
    "           (-0.7369959242164307, 10.697039838804978)]\n",
    "\n",
    "transform = Compose([ToTensor(), \n",
    "                     Lambda(lambda x: GlobalContrastNormalization(x, scale='l1')), \n",
    "                     Normalize([min_max[normal_class][0]], \n",
    "                               [min_max[normal_class][1] - min_max[normal_class][0]])])\n",
    "target_transform = Lambda(lambda x: int(x in outlier_classes))\n",
    "\n",
    "training_data = datasets.MNIST(\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=transform,\n",
    "    target_transform=target_transform,\n",
    ")\n",
    "\n",
    "test_data = datasets.MNIST(\n",
    "    root=\"data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=transform,\n",
    "    target_transform=target_transform,\n",
    ")\n",
    "\n",
    "train_idx_normal = get_target_label_idx(training_data.train_labels.clone().data.cpu().numpy(), normal_classes)\n",
    "train_data = Subset(training_data, train_idx_normal)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-23T04:20:46.184448500Z",
     "start_time": "2023-10-23T04:20:46.009610500Z"
    }
   },
   "id": "6cccc46b60e019d5"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "class MNIST_LeNet_AutoEncoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.rep_dim = 32\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        \n",
    "        # Encoder: Same as Deep Out-of-Context (OOC) network\n",
    "        self.conv1 = nn.Conv2d(1, 8, 5, bias=False, padding=2)\n",
    "        self.bn1 = nn.BatchNorm2d(8, eps=1e-04, affine=False)\n",
    "        self.conv2 = nn.Conv2d(8, 4, 5, bias=False, padding=2)\n",
    "        self.bn2 = nn.BatchNorm2d(4, eps=1e-04, affine=False)\n",
    "        self.fc1 = nn.Linear(4 * 7 * 7, self.rep_dim, bias=False)\n",
    "        \n",
    "        # Decoder\n",
    "        self.deconv1 = nn.ConvTranspose2d(2, 4, 5, bias=False, padding=2)\n",
    "        self.bn3 = nn.BatchNorm2d(4, eps=1e-04, affine=False)\n",
    "        self.deconv2 = nn.ConvTranspose2d(4, 8, 5, bias=False, padding=3)\n",
    "        self.bn4 = nn.BatchNorm2d(8, eps=1e-04, affine=False)\n",
    "        self.deconv3 = nn.ConvTranspose2d(8, 1, 5, bias=False, padding=2)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.pool(nn.functional.leaky_relu(self.bn1(x)))\n",
    "        x = self.conv2(x)\n",
    "        x = self.pool(nn.functional.leaky_relu(self.bn2(x)))\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc1(x)\n",
    "        x = x.view(x.size(0), int(self.rep_dim / 16), 4, 4)\n",
    "        x = nn.functional.interpolate(nn.functional.leaky_relu(x), scale_factor=2)\n",
    "        x = self.deconv1(x)\n",
    "        x = nn.functional.interpolate(nn.functional.leaky_relu(self.bn3(x)), scale_factor=2)\n",
    "        x = self.deconv2(x)\n",
    "        x = nn.functional.interpolate(nn.functional.leaky_relu(self.bn4(x)), scale_factor=2)\n",
    "        x = self.deconv3(x)\n",
    "        x = torch.sigmoid(x)\n",
    "        \n",
    "        return x"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-23T04:20:49.908190500Z",
     "start_time": "2023-10-23T04:20:49.863980900Z"
    }
   },
   "id": "8818e59caa302ac4"
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "class MNIST_LeNet_Network(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.rep_dim = 32\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(1, 8, 5, bias=False, padding=2)\n",
    "        self.bn1 = nn.BatchNorm2d(8, eps=1e-04, affine=False)\n",
    "        self.conv2 = nn.Conv2d(8, 4, 5, bias=False, padding=2)\n",
    "        self.bn2 = nn.BatchNorm2d(4, eps=1e-04, affine=False)\n",
    "        self.fc1 = nn.Linear(4 * 7 * 7, self.rep_dim, bias=False)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.pool(nn.functional.leaky_relu(self.bn1(x)))\n",
    "        x = self.conv2(x)\n",
    "        x = self.pool(nn.functional.leaky_relu(self.bn2(x)))\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc1(x)\n",
    "        return x        "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-23T04:20:51.502608300Z",
     "start_time": "2023-10-23T04:20:51.468565300Z"
    }
   },
   "id": "2d279ab1b1f99e26"
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "# Configuration for Pretrain and Train\n",
    "optimizer_name: str = 'adam'\n",
    "lr: float = 0.001\n",
    "n_epochs: int = 150\n",
    "lr_milestones: tuple = ()\n",
    "batch_size: int = 128\n",
    "weight_decay: float = 1e-6\n",
    "n_jobs_dataloader: int = 0\n",
    "\n",
    "ae_net = MNIST_LeNet_AutoEncoder().to(device)\n",
    "net = MNIST_LeNet_Network().to(device)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-23T04:20:53.056706500Z",
     "start_time": "2023-10-23T04:20:53.009552600Z"
    }
   },
   "id": "e7f0e9725fffc4ed"
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "def AutoEncoder_PreTrain():\n",
    "    logger = logging.getLogger()\n",
    "    \n",
    "    train_loader = DataLoader(train_data, batch_size, num_workers=n_jobs_dataloader)\n",
    "    \n",
    "    # Define loss function and optimizer\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(ae_net.parameters(), lr=lr, weight_decay=weight_decay, amsgrad=(optimizer_name == 'amsgrad'))\n",
    "    schedular = optim.lr_scheduler.MultiStepLR(optimizer, milestones=lr_milestones, gamma=0.1)\n",
    "    \n",
    "    logger.info('Starting pretraining...')\n",
    "    start_time = time.time()\n",
    "    ae_net.train()\n",
    "    \n",
    "    # Training loop\n",
    "    for epoch in range(n_epochs):\n",
    "        schedular.step()\n",
    "        if epoch in lr_milestones:\n",
    "            logger.info('LR Scheduler: new learning rate is %g' % float(schedular.get_lr()[0]))\n",
    "        loss_epoch = 0.0\n",
    "        n_batches = 0\n",
    "        epoch_start_time = time.time()\n",
    "        for data in train_loader:\n",
    "            inputs, _ = data\n",
    "            inputs = inputs.to(device)\n",
    "            \n",
    "            # Zero the network parameters gradients\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Update network parameters via backpropagation: forward + backward + optimize\n",
    "            outputs = ae_net(inputs)\n",
    "            scores = torch.sum((outputs - inputs) ** 2, dim=tuple(range(1, outputs.dim())))\n",
    "            loss = torch.mean(scores)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            loss_epoch += loss.item()\n",
    "            n_batches += 1\n",
    "            \n",
    "        epoch_train_time = time.time() - epoch_start_time\n",
    "        logger.info('Epoch {}/{}\\t Time: {:.3f}\\t Loss: {:.8f}'.format(epoch + 1, n_epochs, epoch_train_time, loss_epoch / n_batches))\n",
    "        \n",
    "    pretrain_time = time.time() - start_time\n",
    "    logger.info('Pretraining time: %.3f' % pretrain_time)\n",
    "    logger.info('Finished pretraining.')\n",
    "        \n",
    "# AutoEncoder_PreTrain()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-23T04:21:02.659946100Z",
     "start_time": "2023-10-23T04:21:02.644331400Z"
    }
   },
   "id": "7400a4e9f9cebfd9"
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "def AutoEncoder_Testing():\n",
    "    from sklearn.metrics import roc_auc_score\n",
    "    from sklearnex import patch_sklearn\n",
    "    patch_sklearn()\n",
    "    \n",
    "    logger = logging.getLogger()\n",
    "    \n",
    "    # Get test data loader\n",
    "    test_loader = DataLoader(test_data, batch_size, num_workers=n_jobs_dataloader)\n",
    "    \n",
    "    # Testing\n",
    "    logger.info('Testing autoencoder...')\n",
    "    loss_epoch = 0.0\n",
    "    n_batches = 0\n",
    "    start_time = time.time()\n",
    "    label_score = []\n",
    "    ae_net.eval()\n",
    "    with torch.no_grad():\n",
    "        for data in test_loader:\n",
    "            inputs, labels = data\n",
    "            inputs = inputs.to(device)\n",
    "            outputs = ae_net(inputs)\n",
    "            scores = torch.sum((outputs - inputs) ** 2, dim=tuple(range(1, outputs.dim())))\n",
    "            loss = torch.mean(scores)\n",
    "    \n",
    "            # Save triple of (idx, label, score) in a list\n",
    "            label_score += list(zip(labels.cpu().data.numpy().tolist(),\n",
    "                                        scores.cpu().data.numpy().tolist()))\n",
    "    \n",
    "            loss_epoch += loss.item()\n",
    "            n_batches += 1\n",
    "    \n",
    "    logger.info('Test set Loss: {:.8f}'.format(loss_epoch / n_batches))\n",
    "    \n",
    "    labels, scores = zip(*label_score)\n",
    "    labels = np.array(labels)\n",
    "    scores = np.array(scores)\n",
    "    \n",
    "    auc = roc_auc_score(labels, scores)\n",
    "    logger.info('Test set AUC: {:.2f}%'.format(100. * auc))\n",
    "    \n",
    "    test_time = time.time() - start_time\n",
    "    logger.info('Autoencoder testing time: %.3f' % test_time)\n",
    "    logger.info('Finished testing autoencoder.')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-23T04:21:07.926445800Z",
     "start_time": "2023-10-23T04:21:07.903312700Z"
    }
   },
   "id": "3290fcd14f890f8a"
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "# Save Pretrain model\n",
    "def save_ae():\n",
    "    ae_net_dict = ae_net.state_dict()\n",
    "    torch.save({'ae_net_dict': ae_net_dict}, 'saved_model/ae_net.tar')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-23T04:21:12.060523100Z",
     "start_time": "2023-10-23T04:21:12.027881100Z"
    }
   },
   "id": "51813de3f443dbb9"
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "# Load Pretrain model\n",
    "def load_ae():\n",
    "    model_dict = torch.load('saved_model/ae_net.tar')\n",
    "    ae_net.load_state_dict(model_dict['ae_net_dict'])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-23T04:21:16.032992500Z",
     "start_time": "2023-10-23T04:21:16.017894900Z"
    }
   },
   "id": "68f19793b3f32d8d"
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 2023-10-23 11:21:27,803 | Testing autoencoder...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Intel(R) Extension for Scikit-learn* enabled (https://github.com/intel/scikit-learn-intelex)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 2023-10-23 11:21:32,820 | Test set Loss: 7.97450212\n",
      "INFO 2023-10-23 11:21:32,831 | Test set AUC: 99.10%\n",
      "INFO 2023-10-23 11:21:32,832 | Autoencoder testing time: 5.027\n",
      "INFO 2023-10-23 11:21:32,833 | Finished testing autoencoder.\n"
     ]
    }
   ],
   "source": [
    "load_ae()\n",
    "AutoEncoder_Testing()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-23T04:21:32.850395300Z",
     "start_time": "2023-10-23T04:21:25.451116400Z"
    }
   },
   "id": "f7318b939f6ff50e"
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "data": {
      "text/plain": "<All keys matched successfully>"
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Init Network Weights from Pretraining\n",
    "# def init_network_weights_from_pretraining():\n",
    "\"\"\"Initialize the Deep SVDD network weights from the encoder weights of the pretraining autoencoder.\"\"\"\n",
    "\n",
    "net_dict = net.state_dict()\n",
    "ae_net_dict = ae_net.state_dict()\n",
    "\n",
    "# Filter out decoder network keys\n",
    "ae_net_dict = {k: v for k, v in ae_net_dict.items() if k in net_dict}\n",
    "# Overwrite values in the existing state_dict\n",
    "net_dict.update(ae_net_dict)\n",
    "# Load the new state_dict\n",
    "net.load_state_dict(net_dict)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-23T04:21:38.019853300Z",
     "start_time": "2023-10-23T04:21:37.951515800Z"
    }
   },
   "id": "8dc0ac95afd4e0f9"
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "# Init Hypersphere\n",
    "R = 0.0     # Hypersphere radius R\n",
    "c = None    # Hypersphere center c\n",
    "\n",
    "R = torch.tensor(R, device=device)\n",
    "# c = torch.tensor(c, device=device)\n",
    "nu: float = 0.1\n",
    "assert (0 < nu) & (nu <= 1), \"For hyperparameter nu, it must hold: 0 < nu <= 1.\"\n",
    "\n",
    "warm_up_n_epochs = 10   # Number of training epochs for soft-boundary Deep SVDD before radius R gets updated\n",
    "\n",
    "objective: str = 'one-class'\n",
    "assert objective in ('one-class', 'soft-boundary'), \"Objective must be either 'one-class' or 'soft-boundary'.\"\n",
    "\n",
    "# Results\n",
    "train_time = None\n",
    "test_auc = None\n",
    "test_time = None\n",
    "test_scores = None"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-23T04:23:30.481140500Z",
     "start_time": "2023-10-23T04:23:30.404172Z"
    }
   },
   "id": "2e20b43a8ff54724"
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [],
   "source": [
    "def init_center_c(train_loader: DataLoader, eps=0.1):\n",
    "    \"\"\"Initialize hypersphere center c as the mean from an initial forward pass on the data.\"\"\"\n",
    "    n_samples = 0\n",
    "    c = torch.zeros(net.rep_dim, device=device)\n",
    "\n",
    "    net.eval()\n",
    "    with torch.no_grad():\n",
    "        for data in train_loader:\n",
    "            # get the inputs of the batch\n",
    "            inputs, _ = data\n",
    "            inputs = inputs.to(device)\n",
    "            outputs = net(inputs)\n",
    "            n_samples += outputs.shape[0]\n",
    "            c += torch.sum(outputs, dim=0)\n",
    "\n",
    "    c /= n_samples\n",
    "\n",
    "    # If c_i is too close to 0, set to +-eps. Reason: a zero unit can be trivially matched with zero weights.\n",
    "    c[(abs(c) < eps) & (c < 0)] = -eps\n",
    "    c[(abs(c) < eps) & (c > 0)] = eps\n",
    "\n",
    "    return c\n",
    "\n",
    "\n",
    "def get_radius(dist: torch.Tensor, nu: float):\n",
    "    \"\"\"Optimally solve for radius R via the (1-nu)-quantile of distances.\"\"\"\n",
    "    return np.quantile(np.sqrt(dist.clone().data.cpu().numpy()), 1 - nu)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-23T04:53:35.606150900Z",
     "start_time": "2023-10-23T04:53:35.585423900Z"
    }
   },
   "id": "c9396757fd2f37d0"
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 2023-10-23 11:53:36,331 | Initializing center c...\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "bad operand type for unary -: 'MNIST_LeNet_Network'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[26], line 16\u001B[0m\n\u001B[0;32m     14\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m c \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m     15\u001B[0m     logger\u001B[38;5;241m.\u001B[39minfo(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mInitializing center c...\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m---> 16\u001B[0m     c \u001B[38;5;241m=\u001B[39m \u001B[43minit_center_c\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtrain_loader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnet\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     17\u001B[0m     logger\u001B[38;5;241m.\u001B[39minfo(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mCenter c initialized.\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m     19\u001B[0m \u001B[38;5;66;03m# Training\u001B[39;00m\n",
      "Cell \u001B[1;32mIn[25], line 19\u001B[0m, in \u001B[0;36minit_center_c\u001B[1;34m(train_loader, eps)\u001B[0m\n\u001B[0;32m     16\u001B[0m c \u001B[38;5;241m/\u001B[39m\u001B[38;5;241m=\u001B[39m n_samples\n\u001B[0;32m     18\u001B[0m \u001B[38;5;66;03m# If c_i is too close to 0, set to +-eps. Reason: a zero unit can be trivially matched with zero weights.\u001B[39;00m\n\u001B[1;32m---> 19\u001B[0m c[(\u001B[38;5;28mabs\u001B[39m(c) \u001B[38;5;241m<\u001B[39m eps) \u001B[38;5;241m&\u001B[39m (c \u001B[38;5;241m<\u001B[39m \u001B[38;5;241m0\u001B[39m)] \u001B[38;5;241m=\u001B[39m \u001B[38;5;241;43m-\u001B[39;49m\u001B[43meps\u001B[49m\n\u001B[0;32m     20\u001B[0m c[(\u001B[38;5;28mabs\u001B[39m(c) \u001B[38;5;241m<\u001B[39m eps) \u001B[38;5;241m&\u001B[39m (c \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0\u001B[39m)] \u001B[38;5;241m=\u001B[39m eps\n\u001B[0;32m     22\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m c\n",
      "\u001B[1;31mTypeError\u001B[0m: bad operand type for unary -: 'MNIST_LeNet_Network'"
     ]
    }
   ],
   "source": [
    "# def train():\n",
    "logger = logging.getLogger()\n",
    "\n",
    "# Get train data loader\n",
    "train_loader = DataLoader(train_data, batch_size, num_workers=n_jobs_dataloader)\n",
    "    \n",
    "# Set optimizer (Adam optimizer for now)\n",
    "optimizer = torch.optim.Adam(ae_net.parameters(), lr=lr, weight_decay=weight_decay, amsgrad=(optimizer_name == 'amsgrad'))\n",
    "\n",
    "# Set learning rate scheduler\n",
    "scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=lr_milestones, gamma=0.1)\n",
    "\n",
    "# Initialize hypersphere center c (if c not loaded)\n",
    "if c is None:\n",
    "    logger.info('Initializing center c...')\n",
    "    c = init_center_c(train_loader, net)\n",
    "    logger.info('Center c initialized.')\n",
    "\n",
    "# Training\n",
    "logger.info('Starting training...')\n",
    "start_time = time.time()\n",
    "net.train()\n",
    "for epoch in range(n_epochs):\n",
    "\n",
    "    scheduler.step()\n",
    "    if epoch in lr_milestones:\n",
    "        logger.info('  LR scheduler: new learning rate is %g' % float(scheduler.get_lr()[0]))\n",
    "\n",
    "    loss_epoch = 0.0\n",
    "    n_batches = 0\n",
    "    epoch_start_time = time.time()\n",
    "    for data in train_loader:\n",
    "        inputs, _ = data\n",
    "        inputs = inputs.to(device)\n",
    "\n",
    "        # Zero the network parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Update network parameters via backpropagation: forward + backward + optimize\n",
    "        outputs = net(inputs)\n",
    "        dist = torch.sum((outputs - c) ** 2, dim=1)\n",
    "        if objective == 'soft-boundary':\n",
    "            scores = dist - R ** 2\n",
    "            loss = R ** 2 + (1 / nu) * torch.mean(torch.max(torch.zeros_like(scores), scores))\n",
    "        else:\n",
    "            loss = torch.mean(dist)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Update hypersphere radius R on mini-batch distances\n",
    "        if (objective == 'soft-boundary') and (epoch >= warm_up_n_epochs):\n",
    "            R.data = torch.tensor(get_radius(dist, nu), device=device)\n",
    "\n",
    "        loss_epoch += loss.item()\n",
    "        n_batches += 1\n",
    "\n",
    "    # log epoch statistics\n",
    "    epoch_train_time = time.time() - epoch_start_time\n",
    "    logger.info('  Epoch {}/{}\\t Time: {:.3f}\\t Loss: {:.8f}'\n",
    "                .format(epoch + 1, n_epochs, epoch_train_time, loss_epoch / n_batches))\n",
    "\n",
    "train_time = time.time() - start_time\n",
    "logger.info('Training time: %.3f' % train_time)\n",
    "\n",
    "logger.info('Finished training.')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-23T04:53:39.411738600Z",
     "start_time": "2023-10-23T04:53:36.329499700Z"
    }
   },
   "id": "a0011a948dc6ddeb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#end train\n",
    "R = float(R.cpu().data.numpy())\n",
    "c = c.cpu().data.numpy().tolist()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3bb776e5ba57056f"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
