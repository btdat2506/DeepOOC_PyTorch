{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-10-20T09:07:58.055266200Z",
     "start_time": "2023-10-20T09:07:57.961537400Z"
    }
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader, Dataset, Subset\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor, Normalize, Compose, Lambda\n",
    "from torch.nn.functional import normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "# Set hardware\n",
    "\n",
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-20T09:07:58.055266200Z",
     "start_time": "2023-10-20T09:07:57.977159Z"
    }
   },
   "id": "f5e3c28b88c23a60"
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "def GlobalContrastNormalization(tensor: torch.tensor, scale='l2'):\n",
    "    assert scale in ('l1', 'l2')\n",
    "    n_features = int(np.prod(tensor.shape))\n",
    "            \n",
    "    tensor = tensor - torch.mean(tensor)\n",
    "    \n",
    "    if (scale == 'l1'):\n",
    "        tensor = tensor / torch.mean(torch.abs(tensor))\n",
    "    \n",
    "    if (scale == 'l2'):\n",
    "        tensor = tensor / torch.sqrt(torch.sum(tensor ** 2) / n_features)\n",
    "    \n",
    "    return tensor"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-20T09:07:58.055266200Z",
     "start_time": "2023-10-20T09:07:57.992816800Z"
    }
   },
   "id": "249fc22063ab1e62"
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "def get_target_label_idx(labels, targets):\n",
    "    \"\"\"\n",
    "    Get the indices of labels that are included in targets.\n",
    "    :param labels: array of labels\n",
    "    :param targets: list/tuple of target labels\n",
    "    :return: list with indices of target labels\n",
    "    \"\"\"\n",
    "    return [idx for idx, label in enumerate(labels) if label in targets]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-20T09:07:58.055266200Z",
     "start_time": "2023-10-20T09:07:58.008401800Z"
    }
   },
   "id": "2469a5861133f5e1"
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "normal_class = 0\n",
    "\n",
    "n_classes = 2\n",
    "normal_classes = tuple([normal_class])\n",
    "outlier_classes = list(range(0, 10))\n",
    "outlier_classes.remove(normal_class)\n",
    "\n",
    "min_max = [(-0.8826567065619495, 9.001545489292527),\n",
    "           (-0.6661464580883915, 20.108062262467364),\n",
    "           (-0.7820454743183202, 11.665100841080346),\n",
    "           (-0.7645772083211267, 12.895051191467457),\n",
    "           (-0.7253923114302238, 12.683235701611533),\n",
    "           (-0.7698501867861425, 13.103278415430502),\n",
    "           (-0.778418217980696, 10.457837397569108),\n",
    "           (-0.7129780970522351, 12.057777597673047),\n",
    "           (-0.8280402650205075, 10.581538445782988),\n",
    "           (-0.7369959242164307, 10.697039838804978)]\n",
    "\n",
    "transform = Compose([ToTensor(), \n",
    "                     Lambda(lambda x: GlobalContrastNormalization(x, scale='l1')), \n",
    "                     Normalize([min_max[normal_class][0]], \n",
    "                               [min_max[normal_class][1] - min_max[normal_class][0]])])\n",
    "target_transform = Lambda(lambda x: int(x in outlier_classes))\n",
    "\n",
    "training_data = datasets.MNIST(\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=transform,\n",
    "    target_transform=target_transform,\n",
    ")\n",
    "\n",
    "test_data = datasets.MNIST(\n",
    "    root=\"data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=transform,\n",
    "    target_transform=target_transform,\n",
    ")\n",
    "\n",
    "train_idx_normal = get_target_label_idx(training_data.train_labels.clone().data.cpu().numpy(), normal_classes)\n",
    "train_data = Subset(training_data, train_idx_normal)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-20T09:07:58.133372200Z",
     "start_time": "2023-10-20T09:07:58.039644700Z"
    }
   },
   "id": "6cccc46b60e019d5"
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "class MNIST_LeNet_AutoEncoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.rep_dim = 32\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        \n",
    "        # Encoder: Same as Deep Out-of-Context (OOC) network\n",
    "        self.conv1 = nn.Conv2d(1, 8, 5, bias=False, padding=2)\n",
    "        self.bn1 = nn.BatchNorm2d(8, eps=1e-04, affine=False)\n",
    "        self.conv2 = nn.Conv2d(8, 4, 5, bias=False, padding=2)\n",
    "        self.bn2 = nn.BatchNorm2d(4, eps=1e-04, affine=False)\n",
    "        self.fc1 = nn.Linear(4 * 7 * 7, self.rep_dim, bias=False)\n",
    "        \n",
    "        # Decoder\n",
    "        self.deconv1 = nn.ConvTranspose2d(2, 4, 5, bias=False, padding=2)\n",
    "        self.bn3 = nn.BatchNorm2d(4, eps=1e-04, affine=False)\n",
    "        self.deconv2 = nn.ConvTranspose2d(4, 8, 5, bias=False, padding=3)\n",
    "        self.bn4 = nn.BatchNorm2d(8, eps=1e-04, affine=False)\n",
    "        self.deconv3 = nn.ConvTranspose2d(8, 1, 5, bias=False, padding=2)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.pool(nn.LeakyReLU(self.bn1(x)))\n",
    "        x = self.conv2(x)\n",
    "        x = self.pool(nn.LeakyReLU(self.bn2(x)))\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc1(x)\n",
    "        x = x.view(x.size(0), int(self.rep_dim / 16), 4, 4)\n",
    "        x = nn.functional.interpolate(nn.LeakyReLU(x), scale_factor=2)\n",
    "        x = self.deconv1(x)\n",
    "        x = nn.functional.interpolate(nn.LeakyReLU(self.bn3(x)), scale_factor=2)\n",
    "        x = self.deconv2(x)\n",
    "        x = nn.functional.interpolate(nn.LeakyReLU(self.bn4(x)), scale_factor=2)\n",
    "        x = self.deconv3(x)\n",
    "        x = torch.sigmoid(x)\n",
    "        \n",
    "        return x"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-20T09:07:58.164627100Z",
     "start_time": "2023-10-20T09:07:58.149003700Z"
    }
   },
   "id": "8818e59caa302ac4"
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "max_pool2d(): argument 'input' (position 1) must be Tensor, not LeakyReLU",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[14], line 56\u001B[0m\n\u001B[0;32m     53\u001B[0m     logger\u001B[38;5;241m.\u001B[39minfo(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mPretraining time: \u001B[39m\u001B[38;5;132;01m%.3f\u001B[39;00m\u001B[38;5;124m'\u001B[39m \u001B[38;5;241m%\u001B[39m pretrain_time)\n\u001B[0;32m     54\u001B[0m     logger\u001B[38;5;241m.\u001B[39minfo(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mFinished pretraining.\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m---> 56\u001B[0m \u001B[43mAutoEncoder_PreTrain\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[1;32mIn[14], line 40\u001B[0m, in \u001B[0;36mAutoEncoder_PreTrain\u001B[1;34m()\u001B[0m\n\u001B[0;32m     37\u001B[0m optimizer\u001B[38;5;241m.\u001B[39mzero_grad()\n\u001B[0;32m     39\u001B[0m \u001B[38;5;66;03m# Update network parameters via backpropagation: forward + backward + optimize\u001B[39;00m\n\u001B[1;32m---> 40\u001B[0m outputs \u001B[38;5;241m=\u001B[39m \u001B[43mae_net\u001B[49m\u001B[43m(\u001B[49m\u001B[43minputs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     41\u001B[0m scores \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39msum((outputs \u001B[38;5;241m-\u001B[39m inputs) \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39m \u001B[38;5;241m2\u001B[39m, dim\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mtuple\u001B[39m(\u001B[38;5;28mrange\u001B[39m(\u001B[38;5;241m1\u001B[39m, outputs\u001B[38;5;241m.\u001B[39mdim())))\n\u001B[0;32m     42\u001B[0m loss \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mmean(scores)\n",
      "File \u001B[1;32m~\\AppData\\Local\\miniconda3\\envs\\Deep_OOC\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1516\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1517\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1518\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32m~\\AppData\\Local\\miniconda3\\envs\\Deep_OOC\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1522\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1523\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1524\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1525\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1526\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1527\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1529\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m   1530\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "Cell \u001B[1;32mIn[13], line 24\u001B[0m, in \u001B[0;36mMNIST_LeNet_AutoEncoder.forward\u001B[1;34m(self, x)\u001B[0m\n\u001B[0;32m     22\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, x):\n\u001B[0;32m     23\u001B[0m     x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconv1(x)\n\u001B[1;32m---> 24\u001B[0m     x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpool\u001B[49m\u001B[43m(\u001B[49m\u001B[43mnn\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mLeakyReLU\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbn1\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     25\u001B[0m     x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconv2(x)\n\u001B[0;32m     26\u001B[0m     x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpool(nn\u001B[38;5;241m.\u001B[39mLeakyReLU(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbn2(x)))\n",
      "File \u001B[1;32m~\\AppData\\Local\\miniconda3\\envs\\Deep_OOC\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1516\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1517\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1518\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32m~\\AppData\\Local\\miniconda3\\envs\\Deep_OOC\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1522\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1523\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1524\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1525\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1526\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1527\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1529\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m   1530\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[1;32m~\\AppData\\Local\\miniconda3\\envs\\Deep_OOC\\lib\\site-packages\\torch\\nn\\modules\\pooling.py:166\u001B[0m, in \u001B[0;36mMaxPool2d.forward\u001B[1;34m(self, input)\u001B[0m\n\u001B[0;32m    165\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m: Tensor):\n\u001B[1;32m--> 166\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mF\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmax_pool2d\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mkernel_size\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstride\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    167\u001B[0m \u001B[43m                        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpadding\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdilation\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mceil_mode\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mceil_mode\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    168\u001B[0m \u001B[43m                        \u001B[49m\u001B[43mreturn_indices\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mreturn_indices\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\AppData\\Local\\miniconda3\\envs\\Deep_OOC\\lib\\site-packages\\torch\\_jit_internal.py:488\u001B[0m, in \u001B[0;36mboolean_dispatch.<locals>.fn\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    486\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m if_true(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m    487\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m--> 488\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m if_false(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32m~\\AppData\\Local\\miniconda3\\envs\\Deep_OOC\\lib\\site-packages\\torch\\nn\\functional.py:791\u001B[0m, in \u001B[0;36m_max_pool2d\u001B[1;34m(input, kernel_size, stride, padding, dilation, ceil_mode, return_indices)\u001B[0m\n\u001B[0;32m    789\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m stride \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    790\u001B[0m     stride \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mjit\u001B[38;5;241m.\u001B[39mannotate(List[\u001B[38;5;28mint\u001B[39m], [])\n\u001B[1;32m--> 791\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmax_pool2d\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mkernel_size\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstride\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpadding\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdilation\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mceil_mode\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[1;31mTypeError\u001B[0m: max_pool2d(): argument 'input' (position 1) must be Tensor, not LeakyReLU"
     ]
    }
   ],
   "source": [
    "optimizer_name: str = 'adam'\n",
    "lr: float = 0.001\n",
    "n_epochs: int = 150\n",
    "lr_milestones: tuple = ()\n",
    "batch_size: int = 128\n",
    "weight_decay: float = 1e-6\n",
    "n_jobs_dataloader: int = 0\n",
    "\n",
    "def AutoEncoder_PreTrain():\n",
    "    logger = logging.getLogger()\n",
    "    \n",
    "    ae_net = MNIST_LeNet_AutoEncoder().to(device)\n",
    "    train_loader, _ = (DataLoader(train_data, batch_size, num_workers=n_jobs_dataloader), DataLoader(test_data, batch_size, num_workers=n_jobs_dataloader))\n",
    "    \n",
    "    # Define loss function and optimizer\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(ae_net.parameters(), lr=lr, weight_decay=weight_decay, amsgrad=(optimizer_name == 'amsgrad'))\n",
    "    schedular = optim.lr_scheduler.MultiStepLR(optimizer, milestones=lr_milestones, gamma=0.1)\n",
    "\n",
    "    logger.info('Starting pretraining...')\n",
    "    start_time = time.time()\n",
    "    ae_net.train()\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(n_epochs):\n",
    "        schedular.step()\n",
    "        if epoch in lr_milestones:\n",
    "            logger.info('LR Scheduler: new learning rate is %g' % float(schedular.get_lr()[0]))\n",
    "        loss_epoch = 0.0\n",
    "        n_batches = 0\n",
    "        epoch_start_time = time.time()\n",
    "        for data in train_loader:\n",
    "            inputs, _ = data\n",
    "            inputs = inputs.to(device)\n",
    "            \n",
    "            # Zero the network parameters gradients\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Update network parameters via backpropagation: forward + backward + optimize\n",
    "            outputs = ae_net(inputs)\n",
    "            scores = torch.sum((outputs - inputs) ** 2, dim=tuple(range(1, outputs.dim())))\n",
    "            loss = torch.mean(scores)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            loss_epoch += loss.item()\n",
    "            n_batches += 1\n",
    "            \n",
    "        epoch_train_time = time.time() - epoch_start_time\n",
    "        logger.info('Epoch {}/{}\\t Time: {:.3f}\\t Loss: {:.8f}'.format(epoch + 1, n_epochs, epoch_train_time, loss_epoch / n_batches))\n",
    "        \n",
    "    pretrain_time = time.time() - start_time\n",
    "    logger.info('Pretraining time: %.3f' % pretrain_time)\n",
    "    logger.info('Finished pretraining.')\n",
    "    \n",
    "AutoEncoder_PreTrain()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-20T09:07:58.591763600Z",
     "start_time": "2023-10-20T09:07:58.180237200Z"
    }
   },
   "id": "7400a4e9f9cebfd9"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class MNIST_LeNet_Network(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.rep_dim = 32\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(1, 8, 5, bias=False, padding=2)\n",
    "        self.bn1 = nn.BatchNorm2d(8, eps=1e-04, affine=False)\n",
    "        self.conv2 = nn.Conv2d(8, 4, 5, bias=False, padding=2)\n",
    "        self.bn2 = nn.BatchNorm2d(4, eps=1e-04, affine=False)\n",
    "        self.fc1 = nn.Linear(4 * 7 * 7, self.rep_dim, bias=False)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.pool(nn.LeakyReLU(self.bn1(x)))\n",
    "        x = self.conv2(x)\n",
    "        x = self.pool(nn.LeakyReLU(self.bn2(x)))\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc1(x)\n",
    "        return x        "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-10-20T09:07:58.576142100Z"
    }
   },
   "id": "d279cbbb63fc00a3"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Have no idea where this code come from\n",
    "\n",
    "\"\"\"\n",
    "optimizer_name: str = 'adam'\n",
    "lr: float = 0.001\n",
    "n_epochs: int = 150\n",
    "lr_milestones: tuple = ()\n",
    "batch_size: int = 128\n",
    "weight_decay: float = 1e-6\n",
    "n_jobs_dataloader: int = 0\n",
    "\n",
    "def AutoEncoder_PreTrain():\n",
    "    ae_net = MNIST_LeNet_AutoEncoder.to(device)\n",
    "    train_loader, _ = DataLoader(train_data, batch_size, n_jobs_dataloader)\n",
    "    \n",
    "    # Define loss function and optimizer\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(ae_net.parameters(), lr=lr, weight_decay=weight_decay, amsgrad= (optimizer_name == 'amsgrad'))\n",
    "    schedular = optim.lr_scheduler.MultiStepLR(optimizer, milestones=lr_milestones, gamma=0.1)\n",
    "    # Number of epochs\n",
    "    epochs = 10\n",
    "    \n",
    "    # Training loop\n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0.0\n",
    "        for images, _ in training_data:\n",
    "            # Move images to the device\n",
    "            images = images.to(device)\n",
    "            \n",
    "            # Zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = ae(images)\n",
    "            loss = criterion(outputs, images)\n",
    "            \n",
    "            # Backward pass and optimize\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "        \n",
    "        # Print loss after each epoch\n",
    "        print(f'Epoch {epoch+1}/{epochs}, Loss: {running_loss/len(training_data)}')\n",
    "\"\"\""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-10-20T09:07:58.576142100Z"
    }
   },
   "id": "8dc0ac95afd4e0f9"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
