{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-10-19T17:01:58.509142700Z",
     "start_time": "2023-10-19T17:01:54.257890800Z"
    }
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader, Dataset, Subset\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor, Normalize, Compose, Lambda\n",
    "from torch.nn.functional import normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "# Set hardware\n",
    "\n",
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-19T17:01:58.509142700Z",
     "start_time": "2023-10-19T17:01:58.488938400Z"
    }
   },
   "id": "f5e3c28b88c23a60"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "def GlobalContrastNormalization(tensor: torch.tensor, scale='l2'):\n",
    "    assert scale in ('l1', 'l2')\n",
    "    n_features = int(np.prod(tensor.shape))\n",
    "            \n",
    "    tensor = tensor - torch.mean(tensor)\n",
    "    \n",
    "    if (scale == 'l1'):\n",
    "        tensor = tensor / torch.mean(torch.abs(tensor))\n",
    "    \n",
    "    if (scale == 'l2'):\n",
    "        tensor = tensor / torch.sqrt(torch.sum(tensor ** 2) / n_features)\n",
    "    \n",
    "    return tensor"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-19T17:01:58.527622500Z",
     "start_time": "2023-10-19T17:01:58.509142700Z"
    }
   },
   "id": "249fc22063ab1e62"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "def get_target_label_idx(labels, targets):\n",
    "    \"\"\"\n",
    "    Get the indices of labels that are included in targets.\n",
    "    :param labels: array of labels\n",
    "    :param targets: list/tuple of target labels\n",
    "    :return: list with indices of target labels\n",
    "    \"\"\"\n",
    "    return [idx for idx, label in enumerate(labels) if label in targets]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-19T17:01:58.587786800Z",
     "start_time": "2023-10-19T17:01:58.527622500Z"
    }
   },
   "id": "2469a5861133f5e1"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\btdat\\AppData\\Local\\miniconda3\\envs\\Deep_OOC\\lib\\site-packages\\torchvision\\datasets\\mnist.py:65: UserWarning: train_labels has been renamed targets\n",
      "  warnings.warn(\"train_labels has been renamed targets\")\n"
     ]
    }
   ],
   "source": [
    "normal_class = 0\n",
    "\n",
    "n_classes = 2\n",
    "normal_classes = tuple([normal_class])\n",
    "outlier_classes = list(range(0, 10))\n",
    "outlier_classes.remove(normal_class)\n",
    "\n",
    "min_max = [(-0.8826567065619495, 9.001545489292527),\n",
    "           (-0.6661464580883915, 20.108062262467364),\n",
    "           (-0.7820454743183202, 11.665100841080346),\n",
    "           (-0.7645772083211267, 12.895051191467457),\n",
    "           (-0.7253923114302238, 12.683235701611533),\n",
    "           (-0.7698501867861425, 13.103278415430502),\n",
    "           (-0.778418217980696, 10.457837397569108),\n",
    "           (-0.7129780970522351, 12.057777597673047),\n",
    "           (-0.8280402650205075, 10.581538445782988),\n",
    "           (-0.7369959242164307, 10.697039838804978)]\n",
    "\n",
    "transform = Compose([ToTensor(), \n",
    "                     Lambda(lambda x: GlobalContrastNormalization(x, scale='l1')), \n",
    "                     Normalize([min_max[normal_class][0]], \n",
    "                               [min_max[normal_class][1] - min_max[normal_class][0]])])\n",
    "target_transform = Lambda(lambda x: int(x in outlier_classes))\n",
    "\n",
    "training_data = datasets.MNIST(\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=transform,\n",
    "    target_transform=target_transform,\n",
    ")\n",
    "\n",
    "test_data = datasets.MNIST(\n",
    "    root=\"data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=transform,\n",
    "    target_transform=target_transform,\n",
    ")\n",
    "\n",
    "train_idx_normal = get_target_label_idx(training_data.train_labels.clone().data.cpu().numpy(), normal_classes)\n",
    "train_data = Subset(training_data, train_idx_normal)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-19T17:01:58.817592500Z",
     "start_time": "2023-10-19T17:01:58.557571500Z"
    }
   },
   "id": "6cccc46b60e019d5"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "class MNIST_LeNet_AutoEncoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.rep_dim = 32\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        \n",
    "        # Encoder: Same as Deep Out-of-Context (OOC) network\n",
    "        self.conv1 = nn.Conv2d(1, 8, 5, bias=False, padding=2)\n",
    "        self.bn1 = nn.BatchNorm2d(8, eps=1e-04, affine=False)\n",
    "        self.conv2 = nn.Conv2d(8, 4, 5, bias=False, padding=2)\n",
    "        self.bn2 = nn.BatchNorm2d(4, eps=1e-04, affine=False)\n",
    "        self.fc1 = nn.Linear(4 * 7 * 7, self.rep_dim, bias=False)\n",
    "        \n",
    "        # Decoder\n",
    "        self.deconv1 = nn.ConvTranspose2d(2, 4, 5, bias=False, padding=2)\n",
    "        self.bn3 = nn.BatchNorm2d(4, eps=1e-04, affine=False)\n",
    "        self.deconv2 = nn.ConvTranspose2d(4, 8, 5, bias=False, padding=3)\n",
    "        self.bn4 = nn.BatchNorm2d(8, eps=1e-04, affine=False)\n",
    "        self.deconv3 = nn.ConvTranspose2d(8, 1, 5, bias=False, padding=2)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.pool(nn.LeakyReLU(self.bn1(x)))\n",
    "        x = self.conv2(x)\n",
    "        x = self.pool(nn.LeakyReLU(self.bn2(x)))\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc1(x)\n",
    "        x = x.view(x.size(0), int(self.rep_dim / 16), 4, 4)\n",
    "        x = nn.functional.interpolate(nn.LeakyReLU(x), scale_factor=2)\n",
    "        x = self.deconv1(x)\n",
    "        x = nn.functional.interpolate(nn.LeakyReLU(self.bn3(x)), scale_factor=2)\n",
    "        x = self.deconv2(x)\n",
    "        x = nn.functional.interpolate(nn.LeakyReLU(self.bn4(x)), scale_factor=2)\n",
    "        x = self.deconv3(x)\n",
    "        x = torch.sigmoid(x)\n",
    "        \n",
    "        return x"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-19T17:01:58.957020700Z",
     "start_time": "2023-10-19T17:01:58.829313400Z"
    }
   },
   "id": "8818e59caa302ac4"
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 3, got 2)",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[8], line 56\u001B[0m\n\u001B[0;32m     53\u001B[0m     logger\u001B[38;5;241m.\u001B[39minfo(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mPretraining time: \u001B[39m\u001B[38;5;132;01m%.3f\u001B[39;00m\u001B[38;5;124m'\u001B[39m \u001B[38;5;241m%\u001B[39m pretrain_time)\n\u001B[0;32m     54\u001B[0m     logger\u001B[38;5;241m.\u001B[39minfo(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mFinished pretraining.\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m---> 56\u001B[0m \u001B[43mAutoEncoder_PreTrain\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[1;32mIn[8], line 33\u001B[0m, in \u001B[0;36mAutoEncoder_PreTrain\u001B[1;34m()\u001B[0m\n\u001B[0;32m     31\u001B[0m epoch_start_time \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mtime()\n\u001B[0;32m     32\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m data \u001B[38;5;129;01min\u001B[39;00m train_loader:\n\u001B[1;32m---> 33\u001B[0m     inputs, _, _ \u001B[38;5;241m=\u001B[39m data\n\u001B[0;32m     34\u001B[0m     inputs \u001B[38;5;241m=\u001B[39m inputs\u001B[38;5;241m.\u001B[39mto(device)\n\u001B[0;32m     36\u001B[0m     \u001B[38;5;66;03m# Zero the network parameters gradients\u001B[39;00m\n",
      "\u001B[1;31mValueError\u001B[0m: not enough values to unpack (expected 3, got 2)"
     ]
    }
   ],
   "source": [
    "optimizer_name: str = 'adam'\n",
    "lr: float = 0.001\n",
    "n_epochs: int = 150\n",
    "lr_milestones: tuple = ()\n",
    "batch_size: int = 128\n",
    "weight_decay: float = 1e-6\n",
    "n_jobs_dataloader: int = 0\n",
    "\n",
    "def AutoEncoder_PreTrain():\n",
    "    logger = logging.getLogger()\n",
    "    \n",
    "    ae_net = MNIST_LeNet_AutoEncoder().to(device)\n",
    "    train_loader, _ = (DataLoader(train_data, batch_size, num_workers=n_jobs_dataloader), DataLoader(test_data, batch_size, num_workers=n_jobs_dataloader))\n",
    "    \n",
    "    # Define loss function and optimizer\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(ae_net.parameters(), lr=lr, weight_decay=weight_decay, amsgrad=(optimizer_name == 'amsgrad'))\n",
    "    schedular = optim.lr_scheduler.MultiStepLR(optimizer, milestones=lr_milestones, gamma=0.1)\n",
    "\n",
    "    logger.info('Starting pretraining...')\n",
    "    start_time = time.time()\n",
    "    ae_net.train()\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(n_epochs):\n",
    "        schedular.step()\n",
    "        if epoch in lr_milestones:\n",
    "            logger.info('LR Scheduler: new learning rate is %g' % float(schedular.get_lr()[0]))\n",
    "        loss_epoch = 0.0\n",
    "        n_batches = 0\n",
    "        epoch_start_time = time.time()\n",
    "        for data in train_loader:\n",
    "            inputs, _, _ = data\n",
    "            inputs = inputs.to(device)\n",
    "            \n",
    "            # Zero the network parameters gradients\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Update network parameters via backpropagation: forward + backward + optimize\n",
    "            outputs = ae_net(inputs)\n",
    "            scores = torch.sum((outputs - inputs) ** 2, dim=tuple(range(1, outputs.dim())))\n",
    "            loss = torch.mean(scores)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            loss_epoch += loss.item()\n",
    "            n_batches += 1\n",
    "            \n",
    "        epoch_train_time = time.time() - epoch_start_time\n",
    "        logger.info('Epoch {}/{}\\t Time: {:.3f}\\t Loss: {:.8f}'.format(epoch + 1, n_epochs, epoch_train_time, loss_epoch / n_batches))\n",
    "        \n",
    "    pretrain_time = time.time() - start_time\n",
    "    logger.info('Pretraining time: %.3f' % pretrain_time)\n",
    "    logger.info('Finished pretraining.')\n",
    "    \n",
    "AutoEncoder_PreTrain()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-19T17:06:43.699190200Z",
     "start_time": "2023-10-19T17:06:43.558127400Z"
    }
   },
   "id": "7400a4e9f9cebfd9"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class MNIST_LeNet_Network(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.rep_dim = 32\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(1, 8, 5, bias=False, padding=2)\n",
    "        self.bn1 = nn.BatchNorm2d(8, eps=1e-04, affine=False)\n",
    "        self.conv2 = nn.Conv2d(8, 4, 5, bias=False, padding=2)\n",
    "        self.bn2 = nn.BatchNorm2d(4, eps=1e-04, affine=False)\n",
    "        self.fc1 = nn.Linear(4 * 7 * 7, self.rep_dim, bias=False)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.pool(nn.LeakyReLU(self.bn1(x)))\n",
    "        x = self.conv2(x)\n",
    "        x = self.pool(nn.LeakyReLU(self.bn2(x)))\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc1(x)\n",
    "        return x        "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-10-19T17:02:00.087006900Z"
    }
   },
   "id": "d279cbbb63fc00a3"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Have no idea where this code come from\n",
    "\n",
    "\"\"\"\n",
    "optimizer_name: str = 'adam'\n",
    "lr: float = 0.001\n",
    "n_epochs: int = 150\n",
    "lr_milestones: tuple = ()\n",
    "batch_size: int = 128\n",
    "weight_decay: float = 1e-6\n",
    "n_jobs_dataloader: int = 0\n",
    "\n",
    "def AutoEncoder_PreTrain():\n",
    "    ae_net = MNIST_LeNet_AutoEncoder.to(device)\n",
    "    train_loader, _ = DataLoader(train_data, batch_size, n_jobs_dataloader)\n",
    "    \n",
    "    # Define loss function and optimizer\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(ae_net.parameters(), lr=lr, weight_decay=weight_decay, amsgrad= (optimizer_name == 'amsgrad'))\n",
    "    schedular = optim.lr_scheduler.MultiStepLR(optimizer, milestones=lr_milestones, gamma=0.1)\n",
    "    # Number of epochs\n",
    "    epochs = 10\n",
    "    \n",
    "    # Training loop\n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0.0\n",
    "        for images, _ in training_data:\n",
    "            # Move images to the device\n",
    "            images = images.to(device)\n",
    "            \n",
    "            # Zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = ae(images)\n",
    "            loss = criterion(outputs, images)\n",
    "            \n",
    "            # Backward pass and optimize\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "        \n",
    "        # Print loss after each epoch\n",
    "        print(f'Epoch {epoch+1}/{epochs}, Loss: {running_loss/len(training_data)}')\n",
    "\"\"\""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-10-19T17:02:00.089050500Z"
    }
   },
   "id": "8dc0ac95afd4e0f9"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
