{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "initial_id",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-10-24T08:24:25.126640500Z",
          "start_time": "2023-10-24T08:24:18.112483800Z"
        },
        "id": "initial_id"
      },
      "outputs": [],
      "source": [
        "import logging\n",
        "import time\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch import nn, optim\n",
        "import torch.nn.functional\n",
        "from torch.utils.data import DataLoader, Dataset, Subset\n",
        "from torchvision import datasets\n",
        "from torchvision.transforms import ToTensor, Normalize, Compose, Lambda\n",
        "from torch.nn.functional import normalize"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "33ad622cc6a8f412",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-10-24T08:24:25.163542100Z",
          "start_time": "2023-10-24T08:24:25.108688600Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "33ad622cc6a8f412",
        "outputId": "940440f9-ef3a-4a37-cead-e002b9b203de"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING 2024-01-20 00:52:50,211 | I will output to terminal\n",
            "WARNING 2024-01-20 00:52:50,212 | FOO\n",
            "INFO 2024-01-20 00:52:50,214 | hello\n",
            "WARNING 2024-01-20 00:52:50,214 | BAR\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "import logging.config\n",
        "\n",
        "logging.config.dictConfig({\n",
        "    \"version\": 1,\n",
        "    \"disable_existing_loggers\": True,\n",
        "    \"formatters\": {\n",
        "        \"default\": {\n",
        "            \"format\": \"{levelname} {asctime} | {message}\",\n",
        "            \"style\": \"{\",\n",
        "        },\n",
        "    },\n",
        "    \"handlers\": {\n",
        "        \"console\": {\n",
        "            \"class\": \"logging.StreamHandler\",\n",
        "            \"formatter\": \"default\",\n",
        "            \"stream\": sys.stdout,\n",
        "        }\n",
        "    },\n",
        "    \"root\": {\n",
        "        \"level\": \"DEBUG\",\n",
        "        \"handlers\": [\"console\"],\n",
        "    },\n",
        "})\n",
        "\n",
        "\n",
        "# Get root logger (all other loggers will be derived from this logger's\n",
        "# properties)\n",
        "logger = logging.getLogger()\n",
        "logger.warning(\"I will output to terminal\")  # No output in notebook, goes to terminal\n",
        "\n",
        "# assuming only a single handler has been setup (seems\n",
        "# to be default in notebook), set that handler to go to stdout.\n",
        "# logger.handlers[0].stream = sys.stdout\n",
        "\n",
        "logger.warning(\"FOO\")  # Prints: WARNING:root:FOO\n",
        "logger.info(\"hello\")\n",
        "\n",
        "# Other loggers derive from the root logger, so you can also do:\n",
        "logger2 = logging.getLogger(\"logger2\")\n",
        "logger2.warning(\"BAR\")  # Prints: WARNING:logger2:BAR"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "f5e3c28b88c23a60",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-10-24T08:24:25.165536200Z",
          "start_time": "2023-10-24T08:24:25.134619300Z"
        },
        "id": "f5e3c28b88c23a60"
      },
      "outputs": [],
      "source": [
        "# Set hardware\n",
        "\n",
        "device = (\n",
        "    \"cuda\"\n",
        "    if torch.cuda.is_available()\n",
        "    else \"mps\"\n",
        "    if torch.backends.mps.is_available()\n",
        "    else \"cpu\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "249fc22063ab1e62",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-10-24T08:24:25.252887300Z",
          "start_time": "2023-10-24T08:24:25.149580400Z"
        },
        "id": "249fc22063ab1e62"
      },
      "outputs": [],
      "source": [
        "def GlobalContrastNormalization(tensor: torch.tensor, scale='l2'):\n",
        "    assert scale in ('l1', 'l2')\n",
        "    n_features = int(np.prod(tensor.shape))\n",
        "\n",
        "    tensor = tensor - torch.mean(tensor)\n",
        "\n",
        "    if (scale == 'l1'):\n",
        "        tensor = tensor / torch.mean(torch.abs(tensor))\n",
        "\n",
        "    if (scale == 'l2'):\n",
        "        tensor = tensor / torch.sqrt(torch.sum(tensor ** 2) / n_features)\n",
        "\n",
        "    return tensor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "2469a5861133f5e1",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-10-24T08:24:25.304757400Z",
          "start_time": "2023-10-24T08:24:25.167533Z"
        },
        "id": "2469a5861133f5e1"
      },
      "outputs": [],
      "source": [
        "def get_target_label_idx(labels, targets):\n",
        "    \"\"\"\n",
        "    Get the indices of labels that are included in targets.\n",
        "    :param labels: array of labels\n",
        "    :param targets: list/tuple of target labels\n",
        "    :return: list with indices of target labels\n",
        "    \"\"\"\n",
        "    return [idx for idx, label in enumerate(labels) if label in targets]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "6cccc46b60e019d5",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-10-24T08:24:25.433468700Z",
          "start_time": "2023-10-24T08:24:25.186487400Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6cccc46b60e019d5",
        "outputId": "0b93d06c-30b6-492a-a990-bcef23e67a06"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\btdat\\miniconda3\\envs\\DeepOOC\\Lib\\site-packages\\torchvision\\datasets\\mnist.py:65: UserWarning: train_labels has been renamed targets\n",
            "  warnings.warn(\"train_labels has been renamed targets\")\n"
          ]
        }
      ],
      "source": [
        "normal_class = 0\n",
        "\n",
        "n_classes = 2\n",
        "normal_classes = tuple([normal_class])\n",
        "outlier_classes = list(range(0, 10))\n",
        "outlier_classes.remove(normal_class)\n",
        "\n",
        "min_max = [(-0.8826567065619495, 9.001545489292527),\n",
        "           (-0.6661464580883915, 20.108062262467364),\n",
        "           (-0.7820454743183202, 11.665100841080346),\n",
        "           (-0.7645772083211267, 12.895051191467457),\n",
        "           (-0.7253923114302238, 12.683235701611533),\n",
        "           (-0.7698501867861425, 13.103278415430502),\n",
        "           (-0.778418217980696, 10.457837397569108),\n",
        "           (-0.7129780970522351, 12.057777597673047),\n",
        "           (-0.8280402650205075, 10.581538445782988),\n",
        "           (-0.7369959242164307, 10.697039838804978)]\n",
        "\n",
        "transform = Compose([ToTensor(),\n",
        "                     Lambda(lambda x: GlobalContrastNormalization(x, scale='l1')),\n",
        "                     Normalize([min_max[normal_class][0]],\n",
        "                               [min_max[normal_class][1] - min_max[normal_class][0]])])\n",
        "target_transform = Lambda(lambda x: int(x in outlier_classes))\n",
        "\n",
        "training_data = datasets.MNIST(\n",
        "    root=\"data\",\n",
        "    train=True,\n",
        "    download=True,\n",
        "    transform=transform,\n",
        "    target_transform=target_transform,\n",
        ")\n",
        "\n",
        "test_data = datasets.MNIST(\n",
        "    root=\"data\",\n",
        "    train=False,\n",
        "    download=True,\n",
        "    transform=transform,\n",
        "    target_transform=target_transform,\n",
        ")\n",
        "\n",
        "# train_normal = training_data[np.where(training_data.targets == normal_class)]\n",
        "\n",
        "train_idx_normal = get_target_label_idx(training_data.train_labels.clone().data.cpu().numpy(), normal_classes)\n",
        "train_data = Subset(training_data, train_idx_normal)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "8818e59caa302ac4",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-10-24T08:24:25.463126Z",
          "start_time": "2023-10-24T08:24:25.303646Z"
        },
        "id": "8818e59caa302ac4"
      },
      "outputs": [],
      "source": [
        "class MNIST_LeNet_AutoEncoder(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        self.rep_dim = 32\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "\n",
        "        # Encoder: Same as Deep Out-of-Context (OOC) network\n",
        "        self.conv1 = nn.Conv2d(1, 8, 5, bias=False, padding=2)\n",
        "        self.bn1 = nn.BatchNorm2d(8, eps=1e-04, affine=False)\n",
        "        self.conv2 = nn.Conv2d(8, 4, 5, bias=False, padding=2)\n",
        "        self.bn2 = nn.BatchNorm2d(4, eps=1e-04, affine=False)\n",
        "        self.fc1 = nn.Linear(4 * 7 * 7, self.rep_dim, bias=False)\n",
        "\n",
        "        # Decoder\n",
        "        self.deconv1 = nn.ConvTranspose2d(2, 4, 5, bias=False, padding=2)\n",
        "        self.bn3 = nn.BatchNorm2d(4, eps=1e-04, affine=False)\n",
        "        self.deconv2 = nn.ConvTranspose2d(4, 8, 5, bias=False, padding=3)\n",
        "        self.bn4 = nn.BatchNorm2d(8, eps=1e-04, affine=False)\n",
        "        self.deconv3 = nn.ConvTranspose2d(8, 1, 5, bias=False, padding=2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.pool(nn.functional.leaky_relu(self.bn1(x)))\n",
        "        x = self.conv2(x)\n",
        "        x = self.pool(nn.functional.leaky_relu(self.bn2(x)))\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.fc1(x)\n",
        "        x = x.view(x.size(0), int(self.rep_dim / 16), 4, 4)\n",
        "        x = nn.functional.interpolate(nn.functional.leaky_relu(x), scale_factor=2)\n",
        "        x = self.deconv1(x)\n",
        "        x = nn.functional.interpolate(nn.functional.leaky_relu(self.bn3(x)), scale_factor=2)\n",
        "        x = self.deconv2(x)\n",
        "        x = nn.functional.interpolate(nn.functional.leaky_relu(self.bn4(x)), scale_factor=2)\n",
        "        x = self.deconv3(x)\n",
        "        x = torch.sigmoid(x)\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "2d279ab1b1f99e26",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-10-24T08:24:25.483424500Z",
          "start_time": "2023-10-24T08:24:25.323669800Z"
        },
        "id": "2d279ab1b1f99e26"
      },
      "outputs": [],
      "source": [
        "class MNIST_LeNet_Network(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        self.rep_dim = 32\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "\n",
        "        self.conv1 = nn.Conv2d(1, 8, 5, bias=False, padding=2)\n",
        "        self.bn1 = nn.BatchNorm2d(8, eps=1e-04, affine=False)\n",
        "        self.conv2 = nn.Conv2d(8, 4, 5, bias=False, padding=2)\n",
        "        self.bn2 = nn.BatchNorm2d(4, eps=1e-04, affine=False)\n",
        "        self.fc1 = nn.Linear(4 * 7 * 7, self.rep_dim, bias=False)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.pool(nn.functional.leaky_relu(self.bn1(x)))\n",
        "        x = self.conv2(x)\n",
        "        x = self.pool(nn.functional.leaky_relu(self.bn2(x)))\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.fc1(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "680258ac",
      "metadata": {},
      "outputs": [],
      "source": [
        "\"\"\" python main.py mnist mnist_LeNet ../log/mnist_test ../data --objective soft-boundary --lr 0.0001 --n_epochs 150 --lr_milestone 50 --batch_size 200 --weight_decay 0.5e-6 --pretrain True --ae_lr 0.0001 --ae_n_epochs 150 --ae_lr_milestone 50 --ae_batch_size 200 --ae_weight_decay 0.5e-3 --normal_class 0; \"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "e7f0e9725fffc4ed",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-10-24T08:24:25.483424500Z",
          "start_time": "2023-10-24T08:24:25.344872700Z"
        },
        "id": "e7f0e9725fffc4ed"
      },
      "outputs": [],
      "source": [
        "# Configuration for Pretrain and Train\n",
        "optimizer_name: str = 'adam'\n",
        "lr: float = 0.0001\n",
        "n_epochs: int = 150\n",
        "lr_milestones = [50]\n",
        "batch_size: int = 200\n",
        "weight_decay: float = 1e-6\n",
        "n_jobs_dataloader: int = 0\n",
        "\n",
        "ae_net = MNIST_LeNet_AutoEncoder().to(device)\n",
        "net = MNIST_LeNet_Network().to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7400a4e9f9cebfd9",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-10-24T08:24:25.483424500Z",
          "start_time": "2023-10-24T08:24:25.373153700Z"
        },
        "id": "7400a4e9f9cebfd9"
      },
      "outputs": [],
      "source": [
        "# def AutoEncoder_PreTrain():\n",
        "logger = logging.getLogger()\n",
        "\n",
        "train_loader = DataLoader(train_data, batch_size, num_workers=n_jobs_dataloader)\n",
        "\n",
        "# Define loss function and optimizer\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = torch.optim.Adam(ae_net.parameters(), lr=lr, weight_decay=weight_decay, amsgrad=(optimizer_name == 'amsgrad'))\n",
        "schedular = optim.lr_scheduler.MultiStepLR(optimizer, milestones=lr_milestones, gamma=0.1)\n",
        "\n",
        "logger.info('Starting pretraining...')\n",
        "start_time = time.time()\n",
        "ae_net.train()\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(n_epochs):\n",
        "    schedular.step()\n",
        "    if epoch in lr_milestones:\n",
        "        logger.info('LR Scheduler: new learning rate is %g' % float(schedular.get_lr()[0]))\n",
        "    loss_epoch = 0.0\n",
        "    n_batches = 0\n",
        "    epoch_start_time = time.time()\n",
        "    for data in train_loader:\n",
        "        inputs, _ = data\n",
        "        inputs = inputs.to(device)\n",
        "\n",
        "        # Zero the network parameters gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Update network parameters via backpropagation: forward + backward + optimize\n",
        "        outputs = ae_net(inputs)\n",
        "        scores = torch.sum((outputs - inputs) ** 2, dim=tuple(range(1, outputs.dim())))\n",
        "        loss = torch.mean(scores)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        loss_epoch += loss.item()\n",
        "        n_batches += 1\n",
        "\n",
        "    epoch_train_time = time.time() - epoch_start_time\n",
        "    logger.info('Epoch {}/{}\\t Time: {:.3f}\\t Loss: {:.8f}'.format(epoch + 1, n_epochs, epoch_train_time, loss_epoch / n_batches))\n",
        "\n",
        "pretrain_time = time.time() - start_time\n",
        "logger.info('Pretraining time: %.3f' % pretrain_time)\n",
        "logger.info('Finished pretraining.')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "GLNmZV5oKAcg",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GLNmZV5oKAcg",
        "outputId": "838c3b83-14ca-46d8-f459-53924f4b44f8"
      },
      "outputs": [],
      "source": [
        "# AutoEncoder_PreTrain()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3290fcd14f890f8a",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-10-24T08:24:25.483424500Z",
          "start_time": "2023-10-24T08:24:25.404642600Z"
        },
        "id": "3290fcd14f890f8a"
      },
      "outputs": [],
      "source": [
        "def AutoEncoder_Testing():\n",
        "    from sklearn.metrics import roc_auc_score\n",
        "\n",
        "    logger = logging.getLogger()\n",
        "\n",
        "    # Get test data loader\n",
        "    test_loader = DataLoader(test_data, batch_size, num_workers=n_jobs_dataloader)\n",
        "\n",
        "    # Testing\n",
        "    logger.info('Testing autoencoder...')\n",
        "    loss_epoch = 0.0\n",
        "    n_batches = 0\n",
        "    start_time = time.time()\n",
        "    label_score = []\n",
        "    ae_net.eval()\n",
        "    with torch.no_grad():\n",
        "        for data in test_loader:\n",
        "            inputs, labels = data\n",
        "            inputs = inputs.to(device)\n",
        "            outputs = ae_net(inputs)\n",
        "            scores = torch.sum((outputs - inputs) ** 2, dim=tuple(range(1, outputs.dim())))\n",
        "            loss = torch.mean(scores)\n",
        "\n",
        "            # Save triple of (idx, label, score) in a list\n",
        "            label_score += list(zip(labels.cpu().data.numpy().tolist(),\n",
        "                                        scores.cpu().data.numpy().tolist()))\n",
        "\n",
        "            loss_epoch += loss.item()\n",
        "            n_batches += 1\n",
        "\n",
        "    logger.info('Test set Loss: {:.8f}'.format(loss_epoch / n_batches))\n",
        "\n",
        "    labels, scores = zip(*label_score)\n",
        "    labels = np.array(labels)\n",
        "    scores = np.array(scores)\n",
        "\n",
        "    auc = roc_auc_score(labels, scores)\n",
        "    logger.info('Test set AUC: {:.2f}%'.format(100. * auc))\n",
        "\n",
        "    test_time = time.time() - start_time\n",
        "    logger.info('Autoencoder testing time: %.3f' % test_time)\n",
        "    logger.info('Finished testing autoencoder.')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1c8c342f",
      "metadata": {},
      "outputs": [],
      "source": [
        "AutoEncoder_Testing()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "51813de3f443dbb9",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-10-24T08:24:25.483424500Z",
          "start_time": "2023-10-24T08:24:25.414967300Z"
        },
        "id": "51813de3f443dbb9"
      },
      "outputs": [],
      "source": [
        "# Save Pretrain model\n",
        "def save_ae():\n",
        "    ae_net_dict = ae_net.state_dict()\n",
        "    torch.save({'ae_net_dict': ae_net_dict}, 'saved_model/ae_net.tar')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "68f19793b3f32d8d",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-10-24T08:24:25.483424500Z",
          "start_time": "2023-10-24T08:24:25.442220300Z"
        },
        "id": "68f19793b3f32d8d"
      },
      "outputs": [],
      "source": [
        "# Load Pretrain model\n",
        "def load_ae():\n",
        "    model_dict = torch.load('saved_model/ae_net.tar')\n",
        "    ae_net.load_state_dict(model_dict['ae_net_dict'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f7318b939f6ff50e",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-10-24T08:26:33.274751Z",
          "start_time": "2023-10-24T08:26:22.112636800Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 426
        },
        "id": "f7318b939f6ff50e",
        "outputId": "f7e0a1e5-4d55-43e6-f7db-83d726ffccd6"
      },
      "outputs": [],
      "source": [
        "save_ae()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "U_HXq131O4DM",
      "metadata": {
        "id": "U_HXq131O4DM"
      },
      "outputs": [],
      "source": [
        "save_ae()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8dc0ac95afd4e0f9",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-10-24T08:26:38.834878900Z",
          "start_time": "2023-10-24T08:26:38.804991Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8dc0ac95afd4e0f9",
        "outputId": "b29c9a69-a7a4-4c23-edc7-0a6c5a5d517e"
      },
      "outputs": [],
      "source": [
        "# Init Network Weights from Pretraining\n",
        "# def init_network_weights_from_pretraining():\n",
        "\"\"\"Initialize the Deep SVDD network weights from the encoder weights of the pretraining autoencoder.\"\"\"\n",
        "\n",
        "net_dict = net.state_dict()\n",
        "ae_net_dict = ae_net.state_dict()\n",
        "\n",
        "# Filter out decoder network keys\n",
        "ae_net_dict = {k: v for k, v in ae_net_dict.items() if k in net_dict}\n",
        "# Overwrite values in the existing state_dict\n",
        "net_dict.update(ae_net_dict)\n",
        "# Load the new state_dict\n",
        "net.load_state_dict(net_dict)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1495b059",
      "metadata": {},
      "outputs": [],
      "source": [
        "objective: str = 'soft-boundary'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "2e20b43a8ff54724",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-10-24T08:27:10.622953500Z",
          "start_time": "2023-10-24T08:27:10.584496700Z"
        },
        "id": "2e20b43a8ff54724"
      },
      "outputs": [],
      "source": [
        "# Init Hypersphere\n",
        "R = 0.0     # Hypersphere radius R\n",
        "c = None    # Hypersphere center c\n",
        "\n",
        "R = torch.tensor(R, device=device)\n",
        "# c = torch.tensor(c, device=device)\n",
        "nu: float = 0.1\n",
        "assert (0 < nu) & (nu <= 1), \"For hyperparameter nu, it must hold: 0 < nu <= 1.\"\n",
        "\n",
        "warm_up_n_epochs = 10   # Number of training epochs for soft-boundary Deep SVDD before radius R gets updated\n",
        "\n",
        "objective: str = 'soft-boundary'\n",
        "assert objective in ('one-class', 'soft-boundary'), \"Objective must be either 'one-class' or 'soft-boundary'.\"\n",
        "\n",
        "# Results\n",
        "train_time = None\n",
        "test_auc = None\n",
        "test_time = None\n",
        "test_scores = None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c9396757fd2f37d0",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-10-24T08:27:11.113440800Z",
          "start_time": "2023-10-24T08:27:11.074551400Z"
        },
        "id": "c9396757fd2f37d0"
      },
      "outputs": [],
      "source": [
        "def init_center_c(train_loader: DataLoader, eps=0.1):\n",
        "    \"\"\"Initialize hypersphere center c as the mean from an initial forward pass on the data.\"\"\"\n",
        "    n_samples = 0\n",
        "    c = torch.zeros(net.rep_dim, device=device)\n",
        "\n",
        "    net.eval()\n",
        "    with torch.no_grad():\n",
        "        for data in train_loader:\n",
        "            # get the inputs of the batch\n",
        "            inputs, _ = data\n",
        "            inputs = inputs.to(device)\n",
        "            outputs = net(inputs)\n",
        "            n_samples += outputs.shape[0]\n",
        "            c += torch.sum(outputs, dim=0)\n",
        "\n",
        "    c /= n_samples\n",
        "\n",
        "    # If c_i is too close to 0, set to +-eps. Reason: a zero unit can be trivially matched with zero weights.\n",
        "    c[(abs(c) < eps) & (c < 0)] = -eps\n",
        "    c[(abs(c) < eps) & (c > 0)] = eps\n",
        "\n",
        "    return c\n",
        "\n",
        "\n",
        "def get_radius(dist: torch.Tensor, nu: float):\n",
        "    \"\"\"Optimally solve for radius R via the (1-nu)-quantile of distances.\"\"\"\n",
        "    return np.quantile(np.sqrt(dist.clone().data.cpu().numpy()), 1 - nu)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a0011a948dc6ddeb",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-10-24T08:45:11.494056200Z",
          "start_time": "2023-10-24T08:27:39.683161Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a0011a948dc6ddeb",
        "outputId": "9316688d-7780-4f36-8060-ac21ebb76ab2"
      },
      "outputs": [],
      "source": [
        "logger = logging.getLogger()\n",
        "\n",
        "# Get train data loader\n",
        "train_loader = DataLoader(train_data, batch_size, num_workers=n_jobs_dataloader)\n",
        "\n",
        "# Set optimizer (Adam optimizer for now)\n",
        "optimizer = torch.optim.Adam(ae_net.parameters(), lr=lr, weight_decay=weight_decay, amsgrad=(optimizer_name == 'amsgrad'))\n",
        "\n",
        "# Set learning rate scheduler\n",
        "scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=lr_milestones, gamma=0.1)\n",
        "\n",
        "# Initialize hypersphere center c (if c not loaded)\n",
        "if c is None:\n",
        "    logger.info('Initializing center c...')\n",
        "    c = init_center_c(train_loader)\n",
        "    logger.info('Center c initialized.')\n",
        "\n",
        "# Training\n",
        "logger.info('Starting training...')\n",
        "start_time = time.time()\n",
        "net.train()\n",
        "for epoch in range(n_epochs):\n",
        "\n",
        "    scheduler.step()\n",
        "    if epoch in lr_milestones:\n",
        "        logger.info('  LR scheduler: new learning rate is %g' % float(scheduler.get_lr()[0]))\n",
        "\n",
        "    loss_epoch = 0.0\n",
        "    n_batches = 0\n",
        "    epoch_start_time = time.time()\n",
        "    for data in train_loader:\n",
        "        inputs, _ = data\n",
        "        inputs = inputs.to(device)\n",
        "\n",
        "        # Zero the network parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Update network parameters via backpropagation: forward + backward + optimize\n",
        "        outputs = net(inputs)\n",
        "        dist = torch.sum((outputs - c) ** 2, dim=1)\n",
        "        if objective == 'soft-boundary':\n",
        "            scores = dist - R ** 2\n",
        "            loss = R ** 2 + (1 / nu) * torch.mean(torch.max(torch.zeros_like(scores), scores))\n",
        "        else:\n",
        "            loss = torch.mean(dist)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Update hypersphere radius R on mini-batch distances\n",
        "        if (objective == 'soft-boundary') and (epoch >= warm_up_n_epochs):\n",
        "            R.data = torch.tensor(get_radius(dist, nu), device=device)\n",
        "\n",
        "        loss_epoch += loss.item()\n",
        "        n_batches += 1\n",
        "\n",
        "    # log epoch statistics\n",
        "    epoch_train_time = time.time() - epoch_start_time\n",
        "    logger.info('  Epoch {}/{}\\t Time: {:.3f}\\t Loss: {:.8f}'\n",
        "                .format(epoch + 1, n_epochs, epoch_train_time, loss_epoch / n_batches))\n",
        "\n",
        "train_time = time.time() - start_time\n",
        "logger.info('Training time: %.3f' % train_time)\n",
        "\n",
        "logger.info('Finished training.')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3bb776e5ba57056f",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-10-24T08:45:31.747638200Z",
          "start_time": "2023-10-24T08:45:31.712052400Z"
        },
        "id": "3bb776e5ba57056f"
      },
      "outputs": [],
      "source": [
        "#end train\n",
        "R = float(R.cpu().data.numpy())\n",
        "c = c.cpu().data.numpy().tolist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5d92d16f8d7fc770",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-10-24T08:27:19.502920600Z",
          "start_time": "2023-10-24T08:27:19.464677Z"
        },
        "id": "5d92d16f8d7fc770"
      },
      "outputs": [],
      "source": [
        "def save_model():\n",
        "    net_dict = net.state_dict()\n",
        "    ae_net_dict = ae_net.state_dict()\n",
        "    torch.save({'R': R, 'c': c, 'net_dict': net_dict, 'ae_net_dict': ae_net_dict}, 'saved_model/model.tar')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c945b45b6ce9944",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-10-24T08:45:40.204488400Z",
          "start_time": "2023-10-24T08:45:40.113337300Z"
        },
        "id": "c945b45b6ce9944"
      },
      "outputs": [],
      "source": [
        "save_model()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "2353aff6627214ac",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-10-24T08:45:45.954560600Z",
          "start_time": "2023-10-24T08:45:45.924786800Z"
        },
        "id": "2353aff6627214ac"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Load model\n",
        "# def load_model():\n",
        "model_dict = torch.load('saved_model/model.tar')\n",
        "\n",
        "R = model_dict['R']\n",
        "c = model_dict['c']\n",
        "net.load_state_dict(model_dict['net_dict'])\n",
        "ae_net.load_state_dict(model_dict['ae_net_dict'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b719b6f8ea065f85",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-10-24T08:45:54.570692400Z",
          "start_time": "2023-10-24T08:45:54.485379900Z"
        },
        "id": "b719b6f8ea065f85"
      },
      "outputs": [],
      "source": [
        "load_model()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4aa9f792",
      "metadata": {},
      "outputs": [],
      "source": [
        "R"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "6d6f3375",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(0.28120137453079225,\n",
              " [-1.2526745796203613,\n",
              "  -1.0768201351165771,\n",
              "  1.0557451248168945,\n",
              "  0.10000000149011612,\n",
              "  0.5143123865127563,\n",
              "  1.017930269241333,\n",
              "  1.5893070697784424,\n",
              "  -0.8477248549461365,\n",
              "  0.807614803314209,\n",
              "  0.9337013363838196,\n",
              "  2.241178035736084,\n",
              "  0.10000000149011612,\n",
              "  -0.5409656167030334,\n",
              "  0.10000000149011612,\n",
              "  0.3968421220779419,\n",
              "  -0.7000905275344849,\n",
              "  0.3597496747970581,\n",
              "  0.3496190905570984,\n",
              "  1.183727741241455,\n",
              "  0.44117531180381775,\n",
              "  1.5687224864959717,\n",
              "  0.6535235047340393,\n",
              "  -0.14391882717609406,\n",
              "  1.2329165935516357,\n",
              "  2.198951244354248,\n",
              "  0.3452918231487274,\n",
              "  -0.7003046870231628,\n",
              "  -0.7080103754997253,\n",
              "  1.2403590679168701,\n",
              "  0.25938501954078674,\n",
              "  0.1374989002943039,\n",
              "  1.208669900894165])"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "R, c"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "53d28eb8a4df5446",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-10-24T08:55:56.388428800Z",
          "start_time": "2023-10-24T08:55:41.779135Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "53d28eb8a4df5446",
        "outputId": "8f476760-b029-487c-c42c-2b16c7e485cd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO 2024-01-20 00:53:25,330 | Starting testing...\n",
            "INFO 2024-01-20 00:53:32,670 | Testing time: 7.338\n",
            "INFO 2024-01-20 00:53:32,689 | Test set AUC: 97.11%\n",
            "INFO 2024-01-20 00:53:32,690 | Finished testing.\n"
          ]
        }
      ],
      "source": [
        "# def test(self, dataset: BaseADDataset, net: BaseNet):\n",
        "from sklearn.metrics import roc_auc_score, accuracy_score\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "\n",
        "R = torch.tensor(R, device=device)\n",
        "c = torch.tensor(c, device=device)\n",
        "\n",
        "logger = logging.getLogger()\n",
        "\n",
        "# Get test data loader\n",
        "test_loader = DataLoader(test_data, batch_size, num_workers=n_jobs_dataloader)\n",
        "\n",
        "# Testing\n",
        "logger.info('Starting testing...')\n",
        "start_time = time.time()\n",
        "label_score = []\n",
        "net.eval()\n",
        "with torch.no_grad():\n",
        "    for data in test_loader:\n",
        "        inputs, labels = data\n",
        "        inputs = inputs.to(device)\n",
        "        outputs = net(inputs)\n",
        "        dist = torch.sum((outputs - c) ** 2, dim=1)\n",
        "        if objective == 'soft-boundary':\n",
        "            scores = dist - R ** 2\n",
        "        else:\n",
        "            scores = dist\n",
        "\n",
        "        # Save triples of (idx, label, score) in a list\n",
        "        label_score += list(zip(labels.cpu().data.numpy().tolist(),\n",
        "                                    scores.cpu().data.numpy().tolist()))\n",
        "\n",
        "test_time = time.time() - start_time\n",
        "logger.info('Testing time: %.3f' % test_time)\n",
        "\n",
        "test_scores = label_score\n",
        "\n",
        "# Compute AUC\n",
        "labels, scores = zip(*label_score)\n",
        "labels = np.array(labels)\n",
        "scores = np.array(scores)\n",
        "\n",
        "test_auc = roc_auc_score(labels, scores)\n",
        "logger.info('Test set AUC: {:.2f}%'.format(100. * test_auc))\n",
        "\n",
        "logger.info('Finished testing.')\n",
        "\n",
        "# Calculate TP, TN, FP, FN\n",
        "y_true = labels\n",
        "y_scores = scores\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "45e44eca",
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "# Combine the labels and scores into a single list of tuples\n",
        "\n",
        "combined = list(zip(labels.tolist(), scores.tolist()))\n",
        "\n",
        "\n",
        "\n",
        "# Specify the file path to save the json file\n",
        "\n",
        "file_path = 'labels_scores.json'\n",
        "\n",
        "\n",
        "\n",
        "# Write the combined array to a json file\n",
        "\n",
        "with open(file_path, 'w') as f:\n",
        "\n",
        "    json.dump(combined, f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "1447737a",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(array([1, 1, 1, ..., 1, 1, 1]),\n",
              " array([0.09933473, 0.1663322 , 0.19953978, ..., 0.03455955, 0.03810013,\n",
              "        0.03239592]))"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "y_true, y_scores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "0c59fee6",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "True Positive: 1083\n",
            "True Negative: 52\n",
            "False Positive: 928\n",
            "False Negative: 7937\n",
            "Precision: 0.5385380407757334\n",
            "Recall: 0.12006651884700666\n",
            "Accuracy: 0.1135\n",
            "F1 Score: 0.1963557247756323\n"
          ]
        }
      ],
      "source": [
        "threshold = 0.0  # Set the threshold for classification\n",
        "y_pred = np.where(y_scores > threshold, 1, 0) # 1: Anomaly, 0: Normal\n",
        "\n",
        "# Swap 0 (Normal) for 1 (True), and 1 (Anomaly) for 0 (False) in y_true and y_pred\n",
        "y_true = np.logical_not(y_true)\n",
        "y_pred = np.logical_not(y_pred)\n",
        "\n",
        "tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
        "\n",
        "# Calculate Precision and Recall\n",
        "precision = tp / (tp + fp)\n",
        "recall = tp / (tp + fn)\n",
        "\n",
        "# Print the results\n",
        "print(\"True Positive:\", tp)\n",
        "print(\"True Negative:\", tn)\n",
        "print(\"False Positive:\", fp)\n",
        "print(\"False Negative:\", fn)\n",
        "print(\"Precision:\", precision)\n",
        "print(\"Recall:\", recall)\n",
        "\n",
        "# Accuracy, F1 \n",
        "accuracy = accuracy_score(y_true, y_pred)\n",
        "f1 = 2 * (precision * recall) / (precision + recall)\n",
        "print(\"Accuracy:\", accuracy)\n",
        "print(\"F1 Score:\", f1)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
